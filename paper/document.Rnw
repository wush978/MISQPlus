%%This is a very basic article template.
%%There is just one section and two subsections.
\documentclass{article}

\begin{document}


\section{Condition and Property of MISQ Filter} \label{sec:condition-property-MISQ-filter}

\subsection{Modeling Time Series} \label{sec:modeling-time-series }

We follow the model of time series in the literature. Suppose $X = (X_1, X_2,
\cdots, X_n)^T$ is the underlying random vector of observed time series, we
assume \begin{eqnarray}
X = \mu + e, \label{eq:model-X}
\end{eqnarray} where $\mu = (\mu_1, \mu_2, \cdots, \mu_n)^T \in \Re^n$ and $e =
(e_1, e_2, \cdots, e_n)^T$ is a zero-mean random vector. 

For $e$, we assume that $e$ has following properties:
\begin{itemize}
\item
  $e_1, e_2, ..., e_n$ are identical and independent distributed (i.i.d.). We
  denote the distribution of $e_i$ as $\varepsilon$. This assumption makes our
  work much more easily and is common in the literature such as %TODO
\item
  $E \varepsilon = 0$. 
\item
  $E \varepsilon^k \mbox{ are existed and bounded for }k=2,3,4$.
\end{itemize}
The moments of $e$ are listed in Table \ref{moment-e}.

\begin{table} 
\centering
\begin{tabular}{ | c | c | }
  $f(e)$ & $Ef(e)$ \\
  \hline
  $e^T e$ & $n M_2$ \\
  $ee^Te$ & $M_3 1_n$ \\
  $e^Tee^Te$ & $n M_4 + n(n-1) M_2^2$ \\
\end{tabular}
\caption{The moments of $e$.} \label{table:moment-e}
\end{table}

For $\mu$, we assume that $\mu$ is continuous which is expressed mathematically
as: \begin{eqnarray}
\mu_i \approx \mu_{i+1}. \label{eq:model-mu}
\end{eqnarray} 

Many readers might not agree with this assumption, but it is existed in many
existed work and needed to handle time series without multiple observations for
each time stamp.

Moving average is a popular techniques to estimate the $\mu$ in model
\ref{eq:model-X}: 
\begin{eqnarray}
\hat{\mu_i} &=& \frac{1}{2h+1}\sum \limits_{k=-h}^h {X_{i+k}} \nonumber \\
&=& \frac{1}{2h+1}\sum \limits_{k=-h}^h {\mu_{i+k} + e_{i+k}} \nonumber \\
&=& \frac{1}{2h+1}\sum \limits_{k=-h}^h {\mu_{i+k}} + \frac{1}{2h+1}\sum \limits_{k=-h}^h {e_{i+k}}.
\end{eqnarray}
Under the i.i.d. assumption of $e_i$, the variance is reduced from $M_2$ to $\frac{1}{2h+1}M_2$, 
so $\hat{\mu_i}$ might be more closer to $\mu$ compared to $X_i$ if $\mu_i \approx
\frac{1}{2h+1}\sum \limits_{k=-h}^h {\mu_{i+k}}$. Therefore, the reader should notice that moving
average also assumes Eq \ref{eq:model-mu} intrinsically.

\subsection{Linear Filtered Time Series}

In this paper, we define the linear filtered time series $\tilde{X}$ as
\begin{eqnarray}
\tilde{X}_i = \left\{ \begin{array} {l} 
\sum \limits_{k=0}^m {\phi_k X_i} \\
0
\end{array} \right. \begin{array} {l}
\mbox{if } i \geq 1 \mbox{ and } i \leq n-m \\
otherwise
\end{array} \label{eq:filter-X} 
\end{eqnarray} We extend the range of subscribe for convenience.

If $\phi = (-1, 1)^T$, the filtered time series is the one in
\ref{MISQ}. If $\phi = (\frac{1}{5}, \frac{1}{5}, \frac{1}{5}, \frac{1}{5},
\frac{1}{5})$, the filtered time series is the moving averaged time series.

To write the formula in matrix form, we let $\Phi$ be a $(n-m) \times n$ matrix
with $i$-th row and $j$-th column entries \begin{eqnarray} 
\Phi_{i,j} &=& \phi_{i-j}. \label{eq:definition-Phi_ij}
\end{eqnarray}
Therefore, the filtered time series is $\Phi X$.

Here, we add a constraint to $\phi$: \begin{eqnarray}
  \sum \limits_{k=0}^m{\phi_k} = 0. \label{eq:model-phi}
\end{eqnarray}

When $m << n$, we combine the Eq \ref{eq:model-mu} and Eq \ref{eq:model-phi}: 
\begin{eqnarray}
  \Phi X &=& \Phi \mu + \Phi e \quad \mbox{according to Eq \ref{eq:model-X}} \nonumber \\
  &\approx& \Phi e. \label{eq:model-Phi-X} 
\end{eqnarray} The last approximation holds because of \begin{eqnarray}
  \sum \limits_{k=0}^m {\phi_{i+k} \mu_{i+k}} &\approx& \sum \limits_{k=0}^m {\phi_{i+k} \mu_i} \nonumber \\
  &=& 0.
\end{eqnarray}

Therefore, we have an approximation of $\Phi e$. To make inference according to
$\Phi e$, we list the moments of $\Phi e$ in Table \ref{table:moment-Phie}. 
We derive moment estimators for $M_2$, $M_3$, and $M_4$.

Note that we define a $n \times n$ matrix $\Lambda$ as $\Phi^T \Phi$ for
convenience.
The vector $diag(\Lambda) = (\Lambda_{1,1}, \Lambda_{2,2}, \cdots,
\Lambda_{n,n})^T \in \Re^n$ is the diagonal vector of $\Lambda$. The
$tr(\Lambda)$ is the trace of $\Lambda$.



\begin{table} 
\centering
\begin{tabular}{ | c | c | }
  $f(e)$ & $Ef(e)$ \\
  \hline
  $e^T \Phi^T \Phi e$ & $M_2 (n-m) \sum \limits_{k=1}^m {\phi_k^2}$ \\
  $\Phi ee^T \Phi^T \Phi e$ & $M_3 \Phi diag(\Lambda) $ \\
  $e^T \Phi^T \Phi e e^T \Phi^T \Phi e$ & $M_4 \sum \limits_{i=1}^n {\Lambda_{i,i}^2} + M_2^2 \sum \limits_{i \neq j} {\Lambda_{i,i} \Lambda_{j,j} + \Lambda_{i,j}^2}$ \\
\end{tabular}
\caption{The moments of $e$.} \label{table:moment-Phie}
\end{table}

The reader should notice that Eq \ref{eq:model-Phi-X} is more likely to be true
in practice if $m$ is small.  

\subsection{MISQ distance estimator and variance}

If there are two time series $S_1$ and $S_2$ in model \ref{eq:model-X} and
model \ref{eq:model-mu}, let $X = S_1 - S_2$ is still in model \ref{eq:model-X}
and model \ref{eq:model-mu}. The sum of square of $\mu^T \mu$ is the
distance between $S_1$ and $S_2$ without the errors.  



\end{document}
